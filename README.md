# Data Migration API

This is a Proof of Concept (PoC) API built with FastAPI to support a large-scale data migration pipeline. It enables loading historical data from CSV files, managing data via REST endpoints, and backing up or restoring table contents in AVRO format. Also, It generate reports using this data and visualize them using Streamlite.

Note: This solution (first part: Load Historical data, backup and restore and data ingestion via API Rest) was implemented using pyspark and mongo too. It is in the folder data-migration-spark-mongo

---

## üöÄ Features

- Load historic data from CSV files into a SQL database
- Receive and validate new data via REST endpoints
- Single and batch transaction support (1 to 1000 rows)
- AVRO export of each table's data
- Table restoration from AVRO backups
- Docker-ready and deployable in isolated environments
- Generate reports via REST endpoint
- Create a dashboard to visualize the reports using Streamlite

---

## üìÅ Project Structure

```
‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îú‚îÄ‚îÄ core/                          # Logging and configuration
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ logger.py                  # Logger configuration
‚îÇ   ‚îú‚îÄ‚îÄ dashboard/                     # Streamlit dashboard for visual reports
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ hiring_dashboard.py        # Streamlit app to visualize hiring reports
‚îÇ   ‚îú‚îÄ‚îÄ exporters/                     # Backup and restore logic (AVRO)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ avro_utils.py              # Utility functions for AVRO read/write
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ backup_departments.py      # AVRO backup logic for departments
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ backup_hired_employees.py  # AVRO backup logic for hired employees
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ backup_jobs.py             # AVRO backup logic for jobs
‚îÇ   ‚îú‚îÄ‚îÄ loaders/                       # Loaders for historical data
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ base_loader.py             # Common logic for all data loaders
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ department_loader.py       # Loader for departments.csv
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ hire_employee_loader.py    # Loader for hired_employees.csv
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ job_loader.py              # Loader for jobs.csv
‚îÇ   ‚îú‚îÄ‚îÄ reports/                       # Report generation logic using PySpark
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ hiring_above_average.py    # Generate report of departments above average hiring
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ hiring_quarterly_report.py # Generate quarterly hiring report by job and department
‚îÇ   ‚îú‚îÄ‚îÄ routers/                       # FastAPI endpoints
|   ‚îú‚îÄ‚îÄ backup_data.py                 # Backup dispatcher for all tables
‚îÇ   ‚îú‚îÄ‚îÄ database.py                    # DB engine/session creation
|   ‚îú‚îÄ‚îÄ load_data.py                   # Load all CSVs into the database
‚îÇ   ‚îú‚îÄ‚îÄ models.py                      # ORM models
‚îÇ   ‚îú‚îÄ‚îÄ restore_data.py                # Restore dispatcher for all tables
‚îÇ   ‚îî‚îÄ‚îÄ schemas.py                     # Pydantic schemas
‚îú‚îÄ‚îÄ data/                              # Historical CSV files
‚îú‚îÄ‚îÄ drivers/                           # JDBC drivers (SQLite)
‚îú‚îÄ‚îÄ backups/                           # Generated AVRO backup files
‚îú‚îÄ‚îÄ utils/                             # Utility scripts
‚îÇ   ‚îî‚îÄ‚îÄ clean_data.py                  # Script to delete all data from DB
‚îú‚îÄ‚îÄ .gitignore                         # Git ignore file
‚îú‚îÄ‚îÄ Dockerfile                         # Docker image definition
‚îú‚îÄ‚îÄ main.py                            # FastAPI entry point
‚îú‚îÄ‚îÄ Makefile                           # Automation of common tasks
‚îú‚îÄ‚îÄ README.md                          # Project documentation
‚îî‚îÄ‚îÄ requirements.txt                   # Python dependencies

```

---

## ‚öôÔ∏è Setup Instructions

### üì¶ 1. Clone the Repository

```bash
git clone git@github.com:angie1590/data_migration_api.git
cd data_migration_api
```

### üêç 2. Create Virtual Environment (Optional if not using Docker)
> You must have installed python 3.10+. Also the library lmaz (brew install xy) and JAVA 11+
```bash
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

### üîß Installing `make` on the Local Environment (Windows/Mac) (Must)
> Required to run `Makefile` commands
```bash
# Windows (con Chocolatey)
choco install make

# Mac
brew install make
```

---

## üê≥ Docker Deployment (Recommended)

>The system is fully dockerized and ready for deployment.

### üîß 1. Build the Docker Image

```bash
make docker-build
```

### ‚ñ∂Ô∏è 2. Run the API Container

```bash
make docker-run
```
Now the API is available at: [http://localhost:8000/docs](http://localhost:8000/docs)

### ‚ñ∂Ô∏è 3. Open a container shell

```bash
make docker-run-shell
```

---

## üìÇ Load Historical Data

### ‚ûï From the Host:

```bash
make load-historical-data
```

### ‚ûï From Docker:

```bash
make docker-load-historical
```

---

## üíæ Backup & Restore (AVRO Format)
> You can backup and restore the three tables: departments, jobs, hired_employees. It is possible to do those process for all of them.

### üîê Backup from Host:

```bash
make backup TABLE=departments
```

### üîê Backup from Docker:

```bash
make docker-backup departments
```

### üîê Backup all tables from Docker:

```bash
make docker-backup-all
```

### ‚ôªÔ∏è Restore from Host:

```bash
make restore TABLE=departments
```

### ‚ôªÔ∏è Restore from Docker:

```bash
make docker-restore TABLE=departments
```

### ‚ôªÔ∏è Restore all tables from Docker:

```bash
make docker-restore-all
```

---
## üìä Streamlit Dashboard

> Visual interface to analyze hiring reports using PySpark and SQLite.

### üöÄ Launch Dashboard from Host:
```bash
make dashboard-create
```

### üöÄ Launch Dashboard from Host:
```bash
make docker-dashboard
```
> The dashboard will be available at: http://localhost:8501
---

## üß™ Clean the Database

### Clean Data from Docker:

```bash
make docker-clean
```

## üõ† Inspecting the SQLite Database inside the Docker Container
To inspect the SQLite database that runs inside the container, a utility Make command is provided:
```bash
make docker-inspect-db
```
This will:

1. Launch an interactive bash shell inside the data-migration-api container.

2. Open the app.db SQLite file using the sqlite3 command-line interface.
### üìå Requirements:
- The container must be running.

## üß™ Generate Batch Data for Testing
> Use this utility to generate realistic test data in .json format for API batch insertions. It includes departments, jobs, and hired_employees.
### üîß From Host:
```bash
make create-bash-files
```

### üê≥ From Docker:
```bash
make docker-create-bash-files
```
> The files will be saved in the folder utils.
---

## ‚úÖ API Endpoints Summary

Visit the Swagger UI for testing endpoints:
[http://localhost:8000/docs](http://localhost:8000/docs)

## API - Data Ingestion

The system also exposes endpoints to insert data either individually or in batch mode into the following tables:

### Insert Individual Records

- **POST /departments**
  - Insert a new department.
  - Body JSON:
    ```json
    {
      "id": 1,
      "department": "Marketing"
    }
    ```

- **POST /jobs**
  - Insert a new job.
  - Body JSON:
    ```json
    {
      "id": 1,
      "job": "Data Engineer"
    }
    ```

- **POST /hired_employees**
  - Insert a new hired employee.
  - Body JSON:
    ```json
    {
      "id": 1,
      "name": "Jane Doe",
      "datetime": "2021-07-01T00:00:00",
      "department_id": 1,
      "job_id": 1
    }
    ```

### Insert Data in batches

- **POST /departments/batch**
  - Insert multiple departments.
  - Body JSON:
    ```json
    [
      {"id": 1, "department": "Marketing"},
      {"id": 2, "department": "Engineering"}
    ]
    ```

- **POST /jobs/batch**
  - Insert multiple jobs.
  - Body JSON:
    ```json
    [
      {"id": 1, "job": "Data Engineer"},
      {"id": 2, "job": "Backend Developer"}
    ]
    ```

- **POST /hired_employees/batch**
  - Insert multiple hired employees.
  - Body JSON:
    ```json
    [
      {
        "id": 1,
        "name": "Jane Doe",
        "datetime": "2021-07-01T00:00:00",
        "department_id": 1,
        "job_id": 1
      },
      {
        "id": 2,
        "name": "John Smith",
        "datetime": "2021-08-01T00:00:00",
        "department_id": 2,
        "job_id": 2
      }
    ]
    ```
### üìà Hiring Reports API

The system provides two main report endpoints backed by PySpark logic and available via FastAPI.

#### `GET /reports/hiring-quarterly`
Returns the number of employees hired per department and job per quarter (Q1‚ÄìQ4) for the year 2021.

- Response Format: JSON
- Columns: department, job, Q1, Q2, Q3, Q4

#### `GET /reports/hiring-above-average`
Returns the departments that hired more employees than the average in 2021.

- Response Format: JSON
- Columns: department_id, department, hired_count

You can test both endpoints using the interactive Swagger interface:
[http://localhost:8000/docs](http://localhost:8000/docs)

---

## üîê Security Considerations

- Codebase scoped to internal POC use
- Run inside Docker to prevent local pollution
- No public endpoints without container isolation

---

## üìÑ License

This project is provided for demonstration purposes only.
